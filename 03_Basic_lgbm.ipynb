{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68af2c8d",
   "metadata": {},
   "source": [
    "**A huge shoutout goes to [Pawel Jankiewicz](https://www.kaggle.com/paweljankiewicz) who in this [thread](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/307288) very generously shared a lot of practical information on approaching a RecSys problem. For people newer to this space like myself this thread has been invaluable.**\n",
    "\n",
    "In fact, much of the code that follows, is based on my understanding of the concepts discussed in the aforementioned thread.\n",
    "\n",
    "NOTE: The code in this notebook will require some files that have been generated in NB 01 & NB 02. These files are used for local validation.\n",
    "\n",
    "NOTE2: I ran this on a VM with 192GB of RAM. You can probably run it with less RAM using a large swap file. Another thing that could be done is to port this to dask (that would likely involve quite a few code changes as dask doesn't support the full pandas API yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024da27d",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc794e1",
   "metadata": {},
   "source": [
    "The other two notebooks have been fine in the sense that I learned a lot about the problem.\n",
    "\n",
    "I got a sense for how candidate genertion feels, what trends might exist in the data, etc.\n",
    "\n",
    "Let's now try implementing a full (albeit small) data processing pipeline where at the end we will train an lgbm model and make a submission.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3bfdb",
   "metadata": {},
   "source": [
    "# Feature (and candidate) Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0b92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8128ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_RUN = True # whether to use the last week as local validation (set created in 01) or use it for training\n",
    "DRY_RUN = False # run on a subset of data, mostly for development\n",
    "\n",
    "SUB_NAME = 'all_vars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0688bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 s, sys: 2.35 s, total: 29.7 s\n",
      "Wall time: 29.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# https://www.kaggle.com/paweljankiewicz/hm-create-dataset-samples\n",
    "\n",
    "if not DRY_RUN:\n",
    "    transactions = pd.read_csv('data/transactions_train.csv', dtype={\"article_id\": \"str\"})\n",
    "    customers = pd.read_csv('data/customers.csv')\n",
    "    articles = pd.read_csv('data/articles.csv', dtype={\"article_id\": \"str\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c0e32",
   "metadata": {},
   "source": [
    "We can skip customers and articles for now, the most heart of the problem are the transactions.\n",
    "\n",
    "We want to use features we generate for up to an including week $t$ to generate predictions for week $t_{t+1}$.\n",
    "\n",
    "Of course, that is just one way to structure the problem. We could treat it as a time series problem where we look at the sequence of weeks $t_1...t_n$ and predict the purchases for week $t_{n+1}$. There are many ways to frame the problem.\n",
    "\n",
    "But we want to do something that would be\n",
    "* simple\n",
    "* has a chance to lending itself well for using gradient boosted trees\n",
    "\n",
    "In line with [the suggestion from Pawel Jankiewicz](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/307288), we will predict baskets. And to make things even simpler, we will aggregate purchases by weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7a52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for sample_repr, sample in [(\"01\", 0.001), (\"1\", 0.01), (\"5\", 0.05)]:\n",
    "#     print(sample)\n",
    "#     customers_sample = customers.sample(int(customers.shape[0]*sample), replace=False)\n",
    "#     customers_sample_ids = set(customers_sample[\"customer_id\"])\n",
    "#     transactions_sample = transactions[transactions[\"customer_id\"].isin(customers_sample_ids)]\n",
    "#     articles_sample_ids = set(transactions_sample[\"article_id\"])\n",
    "#     articles_sample = articles[articles[\"article_id\"].isin(articles_sample_ids)]\n",
    "#     customers_sample.to_csv(f\"data/customers_sample_{sample_repr}.csv.gz\", index=False)\n",
    "#     transactions_sample.to_csv(f\"data/transactions_train_sample_{sample_repr}.csv.gz\", index=False)\n",
    "#     articles_sample.to_csv(f\"data/articles_train_sample_{sample_repr}.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b932914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DRY_RUN:\n",
    "    transactions = pd.read_csv('data/transactions_train_sample_01.csv.gz', dtype={\"article_id\": \"str\"})\n",
    "    customers = pd.read_csv('data/customers_sample_01.csv.gz')\n",
    "    articles = pd.read_csv('data/articles_train_sample_01.csv.gz', dtype={\"article_id\": \"str\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbf5038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db6cda454ed404295907dcb0a1f3cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.06 s, sys: 4.71 s, total: 10.8 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transactions['week'] = pd.to_datetime(transactions.t_dat, format='%Y-%m-%d') \\\n",
    "    .swifter.apply(lambda t: t.year * 100 + t.week) \\\n",
    "    .rank(method='dense') \\\n",
    "    .astype('int')\n",
    "\n",
    "transactions.drop(columns='t_dat', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e6e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.21 s, sys: 297 ms, total: 3.51 s\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_set_week = transactions.week.max()\n",
    "valid_set_week = test_set_week - 1\n",
    "train_set_weeks = set(transactions.week) - set([test_set_week]) - set([valid_set_week])\n",
    "\n",
    "# because of some of the generated features some data for the first 3 weeks will be missing\n",
    "first_three_weeks = transactions.week.sort_values().unique()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53545b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions[transactions.week > 92] # this should correspond to 15 weeks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b122e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using the last week for local validation\n",
    "\n",
    "if VALID_RUN:\n",
    "    transactions = transactions[transactions.week != transactions.week.max()]\n",
    "    transactions.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ced0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['purchased'] = 1 # our positive examples\n",
    "transactions.drop_duplicates(['customer_id', 'article_id', 'week'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e6e118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 s, sys: 2.01 s, total: 19.3 s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bought_i_weeks_ago = transactions.copy()\n",
    "for i in range(1,4):\n",
    "    bought_i_weeks_ago.week += 1\n",
    "    bought_i_weeks_ago[f'bought_{i}_wks_ago'] = 1\n",
    "    \n",
    "    # updating true sales already in transactions with information on whether an article was bought by a customer\n",
    "    # in the previous week\n",
    "    transactions = pd.merge(\n",
    "        transactions,\n",
    "        bought_i_weeks_ago[['customer_id', 'article_id', 'week', f'bought_{i}_wks_ago']],\n",
    "        on=['customer_id', 'article_id', 'week'], how='left'\n",
    "    )\n",
    "\n",
    "    bought_i_weeks_ago.purchased = 0 # negative examples, possibly (or candidates for the test week)\n",
    "    transactions = pd.concat([transactions, bought_i_weeks_ago]) # adding our negative examples\n",
    "    \n",
    "    transactions[f'bought_{i}_wks_ago'].fillna(0, inplace=True)\n",
    "\n",
    "del bought_i_weeks_ago"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe488f9",
   "metadata": {},
   "source": [
    "`transactions` now contains some fake transactions. But if you observe, I am always adding these possibly fake transactions (for instance, negative examples where in fact a purchase was made of that article by a given customer in a given week) at the bottom of the dataframe.\n",
    "\n",
    "The is insight is key. I will be able to remove them by using `drop_duplicates` once I am done with generating negative examples (or data to predict on for the week in the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a18bef18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>week</th>\n",
       "      <th>purchased</th>\n",
       "      <th>bought_1_wks_ago</th>\n",
       "      <th>bought_2_wks_ago</th>\n",
       "      <th>bought_3_wks_ago</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...</td>\n",
       "      <td>0868392003</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...</td>\n",
       "      <td>0879891001</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...</td>\n",
       "      <td>0892794001</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...</td>\n",
       "      <td>0855005001</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...</td>\n",
       "      <td>0551379001</td>\n",
       "      <td>0.038119</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479115</th>\n",
       "      <td>fff7e7674509592818bf453391af43a85eaaac9a52d858...</td>\n",
       "      <td>0624486049</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479116</th>\n",
       "      <td>fff871bf24b40fd1290215414d760afaa69bb164d2b970...</td>\n",
       "      <td>0717490010</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479117</th>\n",
       "      <td>fff871bf24b40fd1290215414d760afaa69bb164d2b970...</td>\n",
       "      <td>0717490058</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479118</th>\n",
       "      <td>fff871bf24b40fd1290215414d760afaa69bb164d2b970...</td>\n",
       "      <td>0717490057</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479119</th>\n",
       "      <td>fff871bf24b40fd1290215414d760afaa69bb164d2b970...</td>\n",
       "      <td>0824995006</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16118124 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               customer_id  article_id  \\\n",
       "0        0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...  0868392003   \n",
       "1        0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...  0879891001   \n",
       "2        0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...  0892794001   \n",
       "3        0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...  0855005001   \n",
       "4        0002f2f1deddc6f2504d96d65fa10ecbb22d7a13092b45...  0551379001   \n",
       "...                                                    ...         ...   \n",
       "4479115  fff7e7674509592818bf453391af43a85eaaac9a52d858...  0624486049   \n",
       "4479116  fff871bf24b40fd1290215414d760afaa69bb164d2b970...  0717490010   \n",
       "4479117  fff871bf24b40fd1290215414d760afaa69bb164d2b970...  0717490058   \n",
       "4479118  fff871bf24b40fd1290215414d760afaa69bb164d2b970...  0717490057   \n",
       "4479119  fff871bf24b40fd1290215414d760afaa69bb164d2b970...  0824995006   \n",
       "\n",
       "            price  sales_channel_id  week  purchased  bought_1_wks_ago  \\\n",
       "0        0.025407                 2    93          1               0.0   \n",
       "1        0.033881                 2    93          1               0.0   \n",
       "2        0.033881                 2    93          1               0.0   \n",
       "3        0.033881                 2    93          1               0.0   \n",
       "4        0.038119                 2    93          1               0.0   \n",
       "...           ...               ...   ...        ...               ...   \n",
       "4479115  0.013542                 1   109          0               1.0   \n",
       "4479116  0.008458                 2   109          0               1.0   \n",
       "4479117  0.008458                 2   109          0               1.0   \n",
       "4479118  0.008458                 2   109          0               1.0   \n",
       "4479119  0.025407                 2   109          0               1.0   \n",
       "\n",
       "         bought_2_wks_ago  bought_3_wks_ago  \n",
       "0                     0.0               0.0  \n",
       "1                     0.0               0.0  \n",
       "2                     0.0               0.0  \n",
       "3                     0.0               0.0  \n",
       "4                     0.0               0.0  \n",
       "...                   ...               ...  \n",
       "4479115               1.0               1.0  \n",
       "4479116               1.0               1.0  \n",
       "4479117               1.0               1.0  \n",
       "4479118               1.0               1.0  \n",
       "4479119               1.0               1.0  \n",
       "\n",
       "[16118124 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96f14baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 724 ms, sys: 336 ms, total: 1.06 s\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transactions = transactions[transactions.week <= test_set_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92a0ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.35 s, sys: 287 ms, total: 6.63 s\n",
      "Wall time: 6.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bestsellers_previous_week = transactions \\\n",
    "    .groupby(['week', 'sales_channel_id'])['article_id'].value_counts() \\\n",
    "    .groupby(['week', 'sales_channel_id']).head(20) \\\n",
    "    .groupby(['week', 'sales_channel_id']).rank(method='dense', ascending=False) \\\n",
    "    .to_frame('bestseller_previous_week_rank').reset_index()\n",
    "\n",
    "bestsellers_previous_week.week += 1\n",
    "\n",
    "bestsellers_previous_week = bestsellers_previous_week[bestsellers_previous_week.week != bestsellers_previous_week.max().week.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "142d9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.merge(transactions, bestsellers_previous_week, on=['week', 'article_id', 'sales_channel_id'], how='left')\n",
    "transactions.bestseller_previous_week_rank.fillna(999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac2046ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 3.47 s, total: 13.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "negative_examples_bestsellers_previous_week = pd.merge( # negative examples AND candidates for test week!\n",
    "    transactions[['customer_id', 'week']].drop_duplicates(),\n",
    "    bestsellers_previous_week, how='left', on='week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6637e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_examples_bestsellers_previous_week = \\\n",
    "    negative_examples_bestsellers_previous_week[\n",
    "    negative_examples_bestsellers_previous_week.week != negative_examples_bestsellers_previous_week.week.min()\n",
    "]\n",
    "negative_examples_bestsellers_previous_week.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0ec6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.8 s, sys: 9.35 s, total: 41.1 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# need to recover the price\n",
    "\n",
    "negative_examples_bestsellers_previous_week.week -= 1\n",
    "negative_examples_bestsellers_previous_week = pd.merge(\n",
    "    negative_examples_bestsellers_previous_week,\n",
    "    # mean of prices across channels per week (prices vary within a week)\n",
    "    transactions[['week', 'article_id', 'price', 'sales_channel_id']].groupby(['week', 'article_id', 'sales_channel_id']).mean().reset_index(),\n",
    "    on=['week', 'article_id', 'sales_channel_id'],\n",
    "    how='left'\n",
    ") \n",
    "negative_examples_bestsellers_previous_week.week += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd771b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.concat([transactions, negative_examples_bestsellers_previous_week])\n",
    "transactions.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8cd7a",
   "metadata": {},
   "source": [
    "Let's generate candidate examples for all the customers in the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcf9df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/sample_submission.csv')[['customer_id']]\n",
    "\n",
    "if DRY_RUN:\n",
    "    test_data = test_data.iloc[:1000]\n",
    "\n",
    "test_data['sales_channel_id'] = 1\n",
    "\n",
    "test_data_2 = test_data.copy()\n",
    "test_data_2['sales_channel_id'] = 2\n",
    "\n",
    "test_data = pd.concat([test_data, test_data_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ee82707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.92 s, sys: 1.96 s, total: 5.89 s\n",
      "Wall time: 5.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# adding bestsellers from last week in the dataset as candidates for all customers in the test set\n",
    "\n",
    "negative_examples_bestsellers_previous_week = \\\n",
    "    negative_examples_bestsellers_previous_week[\n",
    "        negative_examples_bestsellers_previous_week.week == negative_examples_bestsellers_previous_week.week.max()\n",
    "]\n",
    "\n",
    "bestseller_article_info = negative_examples_bestsellers_previous_week.drop_duplicates(['article_id', 'sales_channel_id'])[['article_id', 'sales_channel_id', 'bestseller_previous_week_rank', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47640e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.21 s, sys: 4.78 s, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_candidates = pd.merge( # negative examples AND candidates for week 108!\n",
    "    test_data,\n",
    "    bestseller_article_info,\n",
    "    how='outer',\n",
    "    on='sales_channel_id'\n",
    ")\n",
    "\n",
    "test_candidates['week'] = transactions.week.max()\n",
    "\n",
    "transactions = pd.concat([transactions, test_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91165f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 310 ms, sys: 468 µs, total: 310 ms\n",
      "Wall time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transactions.purchased.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52f3aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    transactions[f'bought_{i}_wks_ago'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d5b44f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 12.7 s, total: 1min 35s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# removing \"fake\", incorrect transactions\n",
    "transactions.drop_duplicates(['customer_id', 'week', 'article_id', 'sales_channel_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ac8f3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197837278, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6def530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 435 ms, sys: 148 ms, total: 582 ms\n",
      "Wall time: 586 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Yet another way to create negative examples, to further differentiate between user preferences.\n",
    "\n",
    "negative_transactions = transactions[transactions.purchased == 1].copy()\n",
    "negative_transactions.purchased = 0\n",
    "\n",
    "for i in range(1,4):\n",
    "    negative_transactions[f'bought_{i}_wks_ago'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ee2ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 520 ms, sys: 7.93 ms, total: 528 ms\n",
      "Wall time: 527 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "negative_transactions.customer_id = negative_transactions.groupby('week')['customer_id'].transform(np.random.permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bd4788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 17 s, total: 1min 42s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transactions = pd.concat([transactions, negative_transactions])\n",
    "transactions.drop_duplicates(['customer_id', 'week', 'article_id', 'sales_channel_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c7b81a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201775487, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7b69c",
   "metadata": {},
   "source": [
    "Let's merge `customers` and `articles` with `transactions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be67bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.drop(columns=['detail_desc', 'prod_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43b980c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 55s, sys: 1min 50s, total: 5min 46s\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transactions = pd.merge(transactions, articles, on='article_id', how='left')\n",
    "transactions = pd.merge(transactions, customers, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c5786e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201775487, 38)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "871d2755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 26.8 s, total: 2min 2s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# we need to sort transactions to create baskets,\n",
    "# where a basket is a sequence of records\n",
    "# representing purchases (and purchase candidates / negative examples)\n",
    "# for a customer for a given week\n",
    "transactions.sort_values(['customer_id', 'week'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a527072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 8s, sys: 1min 3s, total: 3min 11s\n",
      "Wall time: 6min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# operations above are quite expensive -- let's save them so that we can restart from this place\n",
    "# in the notebook if need be\n",
    "\n",
    "transactions.to_pickle(f\"data/transactions_sorted.pkl\")\n",
    "# transactions = pd.read_pickle(f\"data/transactions_sorted.pkl\")\n",
    "# # need to recalculate this if we restarted the kernel and are loading up the data\n",
    "# test_set_week = transactions.week.max()\n",
    "# valid_set_week = test_set_week - 1\n",
    "# train_set_weeks = set(transactions.week) - set([test_set_week]) - set([valid_set_week])\n",
    "# first_three_weeks = transactions.week.sort_values().unique()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b4ebb",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a566dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 1min 6s, total: 2min 13s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transactions = transactions[~transactions.week.isin(set(first_three_weeks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe75d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_candidates = transactions[transactions.week == test_set_week]\n",
    "train_set = transactions[transactions.week != test_set_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f627760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((146239656, 38), (55535831, 38))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape, test_candidates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1638432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.8 s, sys: 43.1 s, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_X = train_set[train_set.week.isin(train_set_weeks)]\n",
    "valid_X = train_set[train_set.week == valid_set_week]\n",
    "\n",
    "train_y = train_X['purchased']\n",
    "valid_y = valid_X['purchased']\n",
    "\n",
    "train_X = train_X.drop(columns='purchased')\n",
    "valid_X = valid_X.drop(columns='purchased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "657e195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "579b3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_candidates.drop(columns='purchased')\n",
    "del test_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b2248a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.4 s, sys: 3.32 s, total: 40.7 s\n",
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_baskets = train_X.groupby(['customer_id', 'week'])['article_id'].count().values\n",
    "valid_baskets = valid_X.groupby(['customer_id', 'week'])['article_id'].count().values\n",
    "test_baskets = test_X.groupby(['customer_id', 'week'])['article_id'].count().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41249e5d",
   "metadata": {},
   "source": [
    "# Vectorize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa5992b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "lightgbm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99dab09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm.sklearn import LGBMRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44b27b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/logicai-io/recsys2019/blob/master/src/recsys/transformers.py\n",
    "# https://github.com/logicai-io/recsys2019/blob/master/src/recsys/vectorizers.py\n",
    "\n",
    "class PandasToRecords(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, *arg):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.to_dict(orient=\"records\")\n",
    "\n",
    "class SparsityFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_nnz=None):\n",
    "        self.min_nnz = min_nnz\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.sparsity = X.getnnz(0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[:, self.sparsity >= self.min_nnz]\n",
    "\n",
    "class PandasToNpArray(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, *arg):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.values.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "991d34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_examples=0):\n",
    "        self.min_examples = min_examples\n",
    "        self.categories = []\n",
    "        \n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            vc = X.iloc[:, i].value_counts()\n",
    "            self.categories.append(vc[vc > self.min_examples].index.tolist())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = {X.columns[i]: pd.Categorical(X.iloc[:, i], categories=self.categories[i]).codes for i in range(X.shape[1])}\n",
    "        return pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38a1a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed vars: 'customer_id', 'article_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f67309d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = ['index_code', 'postal_code', 'sales_channel_id', 'product_type_name', 'product_group_name', \n",
    " 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name',\n",
    " 'index_name', 'index_group_name', 'section_name', 'garment_group_name',\n",
    " 'FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'graphical_appearance_name',\n",
    " 'bestseller_previous_week_rank', 'product_type_no', 'colour_group_code']\n",
    "numerical_variables = ['graphical_appearance_no', 'perceived_colour_value_id', 'perceived_colour_master_id', \n",
    " 'department_no', 'index_group_no', 'section_no', 'garment_group_no', 'age', 'product_code', 'price']\n",
    "# numerical_variables_to_bin = ['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64609858",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    categorical_variables += [f'bought_{i}_wks_ago']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54c78cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        [   \n",
    "            (\n",
    "                \"categorical\",\n",
    "                Categorize(1000),\n",
    "                categorical_variables,\n",
    "            ),\n",
    "            (\n",
    "                \"numerical\",\n",
    "                make_pipeline(PandasToNpArray(), SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "                numerical_variables,\n",
    "            ),\n",
    "#             (\n",
    "#                 \"numerical_to_bin\",\n",
    "#                 KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile'),\n",
    "#                 numerical_variables_to_bin,\n",
    "#             ),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96782094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = make_pipeline(\n",
    "#     ColumnTransformer(\n",
    "#         [   \n",
    "#             (\n",
    "#                 \"categorical\",\n",
    "#                 make_pipeline(PandasToRecords(), DictVectorizer(), SparsityFilter(min_nnz=60)),\n",
    "#                 categorical_variables,\n",
    "#             ),\n",
    "#             (\n",
    "#                 \"numerical\",\n",
    "#                 make_pipeline(PandasToNpArray(), SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "#                 numerical_variables,\n",
    "#             ),\n",
    "#         ]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cb853e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 52s, sys: 34.8 s, total: 4min 27s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_X_vec = pipe.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19fd2ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.77 s, sys: 2.11 s, total: 11.9 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_X_vec = pipe.transform(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "836b406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"dart\",\n",
    "    n_estimators=10,\n",
    "    importance_type='gain',\n",
    "    verbose=10,\n",
    "#     num_leaves=60,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f5ef7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.761435\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.304362\n",
      "[LightGBM] [Debug] init for col-wise cost 1.991427 seconds, init for row-wise cost 11.962770 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 3.148986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Sparse Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 1834\n",
      "[LightGBM] [Info] Number of data points in the train set: 135721072, number of used features: 31\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[1]\tvalid_0's ndcg@1: 0.720481\tvalid_0's ndcg@2: 0.719496\tvalid_0's ndcg@3: 0.72308\tvalid_0's ndcg@4: 0.722894\tvalid_0's ndcg@5: 0.72269\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[2]\tvalid_0's ndcg@1: 0.720926\tvalid_0's ndcg@2: 0.719785\tvalid_0's ndcg@3: 0.723308\tvalid_0's ndcg@4: 0.723115\tvalid_0's ndcg@5: 0.722899\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[3]\tvalid_0's ndcg@1: 0.720481\tvalid_0's ndcg@2: 0.719496\tvalid_0's ndcg@3: 0.723302\tvalid_0's ndcg@4: 0.723123\tvalid_0's ndcg@5: 0.722914\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[4]\tvalid_0's ndcg@1: 0.720481\tvalid_0's ndcg@2: 0.719496\tvalid_0's ndcg@3: 0.723302\tvalid_0's ndcg@4: 0.723123\tvalid_0's ndcg@5: 0.722914\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[5]\tvalid_0's ndcg@1: 0.720481\tvalid_0's ndcg@2: 0.719496\tvalid_0's ndcg@3: 0.723344\tvalid_0's ndcg@4: 0.723165\tvalid_0's ndcg@5: 0.722956\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[6]\tvalid_0's ndcg@1: 0.720481\tvalid_0's ndcg@2: 0.719496\tvalid_0's ndcg@3: 0.723344\tvalid_0's ndcg@4: 0.723165\tvalid_0's ndcg@5: 0.722956\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[7]\tvalid_0's ndcg@1: 0.720481\tvalid_0's ndcg@2: 0.719496\tvalid_0's ndcg@3: 0.723343\tvalid_0's ndcg@4: 0.723165\tvalid_0's ndcg@5: 0.722955\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[8]\tvalid_0's ndcg@1: 0.720481\tvalid_0's ndcg@2: 0.719496\tvalid_0's ndcg@3: 0.723343\tvalid_0's ndcg@4: 0.723165\tvalid_0's ndcg@5: 0.722955\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[9]\tvalid_0's ndcg@1: 0.720481\tvalid_0's ndcg@2: 0.719496\tvalid_0's ndcg@3: 0.723344\tvalid_0's ndcg@4: 0.723165\tvalid_0's ndcg@5: 0.722955\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[10]\tvalid_0's ndcg@1: 0.720481\tvalid_0's ndcg@2: 0.719496\tvalid_0's ndcg@3: 0.723121\tvalid_0's ndcg@4: 0.722937\tvalid_0's ndcg@5: 0.722729\n",
      "CPU times: user 33min 20s, sys: 1min 2s, total: 34min 22s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = ranker.fit(\n",
    "    train_X_vec,\n",
    "    train_y,\n",
    "    group=train_baskets,\n",
    "    eval_set=[(valid_X_vec, valid_y)],\n",
    "    eval_group=[valid_baskets],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f7abb",
   "metadata": {},
   "source": [
    "Learning more about what our model is doing is always very useful!\n",
    "\n",
    "This can inform what we might want to work on next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6255e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestseller_previous_week_rank\n",
      "price\n",
      "age\n",
      "index_code\n",
      "department_no\n",
      "garment_group_name\n",
      "sales_channel_id\n",
      "product_type_name\n",
      "perceived_colour_value_name\n",
      "colour_group_name\n",
      "section_no\n",
      "product_group_name\n",
      "club_member_status\n",
      "product_code\n",
      "department_name\n",
      "postal_code\n",
      "section_name\n",
      "graphical_appearance_no\n",
      "garment_group_no\n",
      "index_group_name\n",
      "perceived_colour_master_id\n",
      "perceived_colour_master_name\n",
      "index_name\n",
      "Active\n",
      "FN\n",
      "index_group_no\n",
      "graphical_appearance_name\n",
      "product_type_no\n",
      "colour_group_code\n",
      "bought_1_wks_ago\n",
      "bought_2_wks_ago\n",
      "bought_3_wks_ago\n",
      "perceived_colour_value_id\n",
      "fashion_news_frequency\n"
     ]
    }
   ],
   "source": [
    "variables = categorical_variables + numerical_variables # + numerical_variables_to_bin\n",
    "\n",
    "for i in ranker.feature_importances_.argsort()[::-1]:\n",
    "    print(variables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6212e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_set, valid_X, train_X, valid_X_vec, train_X_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "926a2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# # save model\n",
    "# # joblib.dump(ranker, 'data/ranker.pkl')\n",
    "# # load model\n",
    "# ranker = joblib.load('data/ranker.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7cd70eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 42s, sys: 14 s, total: 1min 56s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chunk_size = 5_000_000\n",
    "preds = []\n",
    "\n",
    "for i in range(0, test_X.shape[0], chunk_size):\n",
    "    test_X_chunk_vectorized = pipe.transform(test_X.iloc[i:i+chunk_size])\n",
    "    preds.append(ranker.predict(test_X_chunk_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a0c4a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds)\n",
    "\n",
    "test_X['preds'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "afc38b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X[['customer_id', 'article_id', 'preds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0b51c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del preds, test_X_chunk_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b086e89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 10s, sys: 4.27 s, total: 2min 15s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cust_id2pred = {}\n",
    "\n",
    "for grp in test_X[['customer_id', 'article_id', 'preds']].sort_values(['customer_id', 'preds'], ascending=False).groupby('customer_id'):\n",
    "    cust_id2pred[grp[0]] = grp[1]['article_id'].head(12).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b205f8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 428 ms, total: 16.2 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not DRY_RUN:\n",
    "    sub = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "    preds_str = []\n",
    "    for c in sub.customer_id:\n",
    "        preds_str.append(' '.join(cust_id2pred[c]))\n",
    "\n",
    "    sub.prediction = preds_str\n",
    "    sub.to_csv(f'data/subs/{SUB_NAME}.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b9c78f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 53.3M/53.3M [00:03<00:00, 14.5MB/s]\n",
      "Successfully submitted to H&M Personalized Fashion Recommendations"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c h-and-m-personalized-fashion-recommendations -f 'data/subs/{SUB_NAME}.csv.gz' -m {SUB_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e445ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eval_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b78b6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14255287342912734"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sub(f'data/subs/{SUB_NAME}.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56ec9b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007167646336415192"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sub(f'data/subs/{SUB_NAME}.csv.gz', skip_cust_with_no_purchases=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
